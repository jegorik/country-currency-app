{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11245bca",
   "metadata": {},
   "source": [
    "# Country Currency Data Loading Notebook\n",
    "\n",
    "## Overview\n",
    "This notebook provides a comprehensive ETL (Extract, Transform, Load) pipeline for processing country-to-currency mapping data. It reads data from a CSV file stored in a Databricks volume and loads it into a Delta table with proper data quality checks and validation.\n",
    "\n",
    "## Key Features\n",
    "- **Modular Design**: Functions are organized for reusability and testing\n",
    "- **Data Quality Checks**: Validates data integrity before loading\n",
    "- **Error Handling**: Comprehensive error handling with detailed logging\n",
    "- **Cross-Platform Support**: Works with both Windows and Linux environments\n",
    "- **Parameterized Execution**: Uses Databricks widgets for flexible configuration\n",
    "\n",
    "## Input Parameters\n",
    "- **catalog_name**: The catalog where the schema exists (use 'hive_metastore' for trial accounts)\n",
    "- **schema_name**: The schema where the table will be created\n",
    "- **table_name**: The name of the target Delta table\n",
    "- **csv_path**: Full path to the CSV file in the Databricks volume\n",
    "- **warehouse_id**: SQL warehouse ID for table operations (optional)\n",
    "- **warehouse_name**: SQL warehouse name for reference (optional)\n",
    "\n",
    "## Output Table Schema\n",
    "The output Delta table includes all columns from the source CSV file:\n",
    "- `country_code` (STRING): ISO 3166-1 alpha-3 country code\n",
    "- `country_number` (INT): ISO 3166-1 numeric country code\n",
    "- `country` (STRING): Full country name\n",
    "- `currency_name` (STRING): Official currency name\n",
    "- `currency_code` (STRING): ISO 4217 currency code\n",
    "- `currency_number` (INT): ISO 4217 numeric currency code\n",
    "\n",
    "## Data Quality Checks\n",
    "- Validates presence of key columns (country_code, currency_code)\n",
    "- Checks for null values in critical fields\n",
    "- Verifies data load completeness\n",
    "- Provides detailed logging of any issues found\n",
    "\n",
    "## Usage\n",
    "1. Ensure the CSV file is uploaded to the specified volume path\n",
    "2. Configure the input parameters using the notebook widgets\n",
    "3. Run all cells to execute the complete ETL pipeline\n",
    "4. Review the output logs to confirm successful data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338d2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from typing import Dict, Tuple, List, Optional, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to make the code more modular and testable\n",
    "def validate_parameters(params: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Validate input parameters to ensure all required fields are present.\n",
    "    \n",
    "    Args:\n",
    "        params: Dictionary containing input parameters\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If any required parameter is missing\n",
    "    \"\"\"\n",
    "    required_params = [\"catalog_name\", \"schema_name\", \"table_name\", \"csv_path\"]\n",
    "    missing = [p for p in required_params if not params.get(p)]\n",
    "    \n",
    "    if missing:\n",
    "        error_msg = f\"Missing required parameters: {', '.join(missing)}\"\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "        raise ValueError(error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fcb929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_table_name(params: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Construct the fully qualified table name with proper quoting.\n",
    "    \n",
    "    Args:\n",
    "        params: Dictionary containing catalog_name, schema_name, and table_name\n",
    "        \n",
    "    Returns:\n",
    "        String with properly formatted table name\n",
    "    \"\"\"\n",
    "    return f\"`{params['catalog_name']}`.`{params['schema_name']}`.`{params['table_name']}`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_data(spark_session: Any, csv_path: str) -> Any:\n",
    "    \"\"\"\n",
    "    Read CSV data into a Spark DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        spark_session: Active Spark session\n",
    "        csv_path: Path to the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing the CSV data\n",
    "    \"\"\"\n",
    "    df = spark_session.read.csv(\n",
    "        csv_path,\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "        sep=\",\",\n",
    "        nullValue=\"\",\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b498cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_data_quality_checks(df: Any) -> Tuple[List[Tuple[str, int]], int]:\n",
    "    \"\"\"\n",
    "    Perform data quality checks on the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to check\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing null counts and total record count\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the DataFrame is empty\n",
    "    \"\"\"\n",
    "    # Check for nulls in key columns\n",
    "    key_columns = [\"country_code\", \"currency_code\"]\n",
    "    null_counts = [(col_name, df.filter(df[col_name].isNull()).count()) \n",
    "                  for col_name in key_columns]\n",
    "    \n",
    "    # Get total record count\n",
    "    record_count = df.count()\n",
    "    \n",
    "    if record_count == 0:\n",
    "        raise ValueError(\"CSV file contains no data to load\")\n",
    "        \n",
    "    return null_counts, record_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8648274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_delta_table(df: Any, full_table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Write DataFrame to a Delta table.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        full_table_name: Full table name with catalog and schema\n",
    "    \"\"\"\n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(full_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_load(spark_session: Any, full_table_name: str, expected_count: int) -> int:\n",
    "    \"\"\"\n",
    "    Validate that data was loaded correctly.\n",
    "    \n",
    "    Args:\n",
    "        spark_session: Active Spark session\n",
    "        full_table_name: Full table name with catalog and schema\n",
    "        expected_count: Expected number of rows\n",
    "        \n",
    "    Returns:\n",
    "        Actual number of rows loaded\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If no data was loaded\n",
    "    \"\"\"\n",
    "    count_df = spark_session.sql(f\"SELECT COUNT(*) as row_count FROM {full_table_name}\")\n",
    "    row_count = count_df.collect()[0]['row_count']\n",
    "    \n",
    "    if row_count == 0:\n",
    "        raise Exception(\"No data was loaded into the table\")\n",
    "        \n",
    "    return row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input parameters as widgets\n",
    "dbutils.widgets.text(\"catalog_name\", \"\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"table_name\", \"\", \"Table Name\")\n",
    "dbutils.widgets.text(\"csv_path\", \"\", \"CSV Path\")\n",
    "dbutils.widgets.text(\"warehouse_id\", \"\", \"SQL Warehouse ID (optional)\")\n",
    "dbutils.widgets.text(\"warehouse_name\", \"\", \"SQL Warehouse Name (optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d66f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters_from_widgets():\n",
    "    \"\"\"\n",
    "    Get parameters from notebook widgets.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing parameters from widgets\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"catalog_name\": dbutils.widgets.get(\"catalog_name\"),\n",
    "        \"schema_name\": dbutils.widgets.get(\"schema_name\"),\n",
    "        \"table_name\": dbutils.widgets.get(\"table_name\"),\n",
    "        \"csv_path\": dbutils.widgets.get(\"csv_path\"),\n",
    "        \"warehouse_id\": dbutils.widgets.get(\"warehouse_id\"),\n",
    "        \"warehouse_name\": dbutils.widgets.get(\"warehouse_name\")\n",
    "    }\n",
    "\n",
    "# Get parameters from widgets\n",
    "params = get_parameters_from_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d04229",
   "metadata": {},
   "source": [
    "## Parameter Validation\n",
    "Verify that all required parameters are provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate parameters\n",
    "validate_parameters(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a0335",
   "metadata": {},
   "source": [
    "## Main Execution Function\n",
    "Orchestrates the data loading process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88aff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(params):\n",
    "    \"\"\"\n",
    "    Main execution function that orchestrates the data loading process.\n",
    "    \n",
    "    Args:\n",
    "        params: Dictionary containing input parameters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Log parameters for debugging and auditing\n",
    "        print(f\"Starting data loading process with parameters:\")\n",
    "        print(f\"Catalog: {params['catalog_name']}\")\n",
    "        print(f\"Schema: {params['schema_name']}\")\n",
    "        print(f\"Table: {params['table_name']}\")\n",
    "        print(f\"CSV Path: {params['csv_path']}\")\n",
    "        if params.get('warehouse_name'):\n",
    "            print(f\"Warehouse: {params['warehouse_name']}\")\n",
    "        \n",
    "        # Get full table name\n",
    "        full_table_name = get_full_table_name(params)\n",
    "        \n",
    "        print(\"Reading CSV data...\")\n",
    "        df = read_csv_data(spark, params['csv_path'])\n",
    "        \n",
    "        # Show sample data for verification\n",
    "        print(\"Sample data from CSV:\")\n",
    "        df.show(5, truncate=False)\n",
    "        \n",
    "        # Perform data quality checks\n",
    "        print(\"Performing data quality checks...\")\n",
    "        null_counts, record_count = perform_data_quality_checks(df)\n",
    "        \n",
    "        for col_name, count in null_counts:\n",
    "            if count > 0:\n",
    "                print(f\"WARNING: Found {count} rows with null {col_name}\")\n",
    "        \n",
    "        print(f\"Total records to load: {record_count}\")\n",
    "        \n",
    "        # Write data to the table using Delta format\n",
    "        print(f\"Writing data to table: {full_table_name}\")\n",
    "        write_to_delta_table(df, full_table_name)\n",
    "        \n",
    "        # Verify the data was loaded correctly\n",
    "        print(\"Validating data load...\")\n",
    "        row_count = validate_data_load(spark, full_table_name, record_count)\n",
    "        print(f\"Successfully loaded {row_count} rows into {full_table_name}\")\n",
    "        \n",
    "        if row_count != record_count:\n",
    "            print(f\"WARNING: Record count mismatch. CSV had {record_count} rows, but table has {row_count} rows\")\n",
    "        \n",
    "        result = {\n",
    "            \"status\": \"success\",\n",
    "            \"rows_processed\": row_count,\n",
    "            \"table_name\": full_table_name\n",
    "        }\n",
    "        \n",
    "        print(f\"Job completed successfully with result: {result}\")\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        print(f\"ERROR: Failed to load data: {error_message}\")\n",
    "        raise Exception(f\"Failed to load data: {error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8247de",
   "metadata": {},
   "source": [
    "### Execute Main Data Processing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec8099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the main function which will orchestrate the entire data loading process\n",
    "result = main(params)\n",
    "print(\"Data load process completed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
